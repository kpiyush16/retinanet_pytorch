{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Will Help a lot when trying to inference from the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os, copy, argparse, collections, sys, numpy as np\n",
    "\n",
    "import torch, torch.nn as nn, torch.optim as optim, torchvision\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import model\n",
    "from anchors import Anchors\n",
    "from datagen import CSVDataset, collater, Resizer, AspectRatioBasedSampler, Augmenter, UnNormalizer, Normalizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import csv_eval\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "assert torch.__version__.split('.')[1] == '4'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If loading the full model as in given drive link in repo\n",
    "retinanet = torch.load(\"new_ckpts/50_retinanet_7.pt\")\n",
    "retinanet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If want to load a state_dict from the training scenario in new_train.py\n",
    "\n",
    "# checkpoint = torch.load(\"models/50_retinanet_2.pt\")\n",
    "# retinanet = model.resnet50(num_classes=dataset_test.num_classes(), pretrained=True)\n",
    "# retinanet.load_state_dict(checkpoint['model_state_dict'])\n",
    "# retinanet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retinanet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on Test set Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inference on Test set Images\n",
    "\n",
    "unnormalize = UnNormalizer()\n",
    "\n",
    "def draw_caption(image, box, caption):\n",
    "    b = np.array(box).astype(int)\n",
    "    cv2.putText(image, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.5, color=(0, 0, 255), thickness=2)\n",
    "\n",
    "\n",
    "def draw_from_images(dataloader_test):\n",
    "    COLORS = np.random.uniform(0, 255, size=(dataset_test.num_classes(), 3))\n",
    "    res = dict((v,k) for k,v in dataset_test.labels.items())\n",
    "    for idx, data in enumerate(dataloader_test):\n",
    "        if(idx > 20):\n",
    "            break\n",
    "        with torch.no_grad():\n",
    "            st = time.time()\n",
    "            print(data['img'].shape)\n",
    "            scores, classification, transformed_anchors = retinanet(data['img'].cuda().float())\n",
    "            print('Elapsed time: {}'.format(time.time()-st))\n",
    "            print(type(data['img'].cuda().float()))\n",
    "            print(data['img'].shape)\n",
    "            print(data['img'].cuda().float().shape)\n",
    "\n",
    "            idxs = np.where(scores>0.5)\n",
    "            img = np.array(255 * unnormalize(data['img'][0, :, :, :])).copy()\n",
    "\n",
    "            img[img<0] = 0\n",
    "            img[img>255] = 255\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "    #         print(img.shape)\n",
    "\n",
    "            img = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "    #         print(img.shape)\n",
    "    #         img = img.astype(np.uint8)\n",
    "\n",
    "            for j in range(idxs[0].shape[0]):\n",
    "                bbox = transformed_anchors[idxs[0][j], :]\n",
    "                x1 = int(bbox[0])\n",
    "                y1 = int(bbox[1])\n",
    "                x2 = int(bbox[2])\n",
    "                y2 = int(bbox[3])\n",
    "                label_name = dataset_test.labels[int(classification[idxs[0][j]])]\n",
    "                color = COLORS[res[label_name]]\n",
    "\n",
    "                draw_caption(img, (x1, y1, x2, y2), label_name)\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), color, thickness=2)\n",
    "\n",
    "            fig = plt.figure(figsize=(13.66,768))\n",
    "            img = img[:,:,::-1]\n",
    "            plt.imshow(img)\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = CSVDataset(train_file=\"data/test/test_annot.csv\", class_list=\"data/class_ids.txt\", transform=transforms.Compose([Normalizer(), Resizer()]))\n",
    "\n",
    "dataloader_test = DataLoader(dataset_test, num_workers=2, collate_fn=collater)\n",
    "\n",
    "draw_from_images(dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resizer(image, min_side=608, max_side=1024):\n",
    "    rows, cols, cns = image.shape\n",
    "    smallest_side = min(rows, cols)\n",
    "\n",
    "    # rescale the image so the smallest side is min_side\n",
    "    scale = min_side / smallest_side\n",
    "\n",
    "    # check if the largest side is now greater than max_side, which can happen\n",
    "    # when images have a large aspect ratio\n",
    "    largest_side = max(rows, cols)\n",
    "\n",
    "    if largest_side * scale > max_side:\n",
    "        scale = max_side / largest_side\n",
    "    \n",
    "    image = cv2.resize(image, (int(round(cols*scale)), int(round((rows*scale)))))\n",
    "    rows, cols, cns = image.shape\n",
    "\n",
    "    pad_w = 32 - rows%32\n",
    "    pad_h = 32 - cols%32\n",
    "    new_image = np.zeros((rows + pad_w, cols + pad_h, cns)).astype(np.uint8)\n",
    "    new_image[:rows, :cols, :] = image.astype(np.uint8)\n",
    "    return new_image\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406],\n",
    "   std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "transform=transforms.Compose([transforms.ToPILImage(), transforms.ToTensor(), normalize])\n",
    "def camera(file, SaveVideo=1):\n",
    "    COLORS = np.random.uniform(150, 255, size=(dataset_test.num_classes(), 3))\n",
    "    res = dict((v,k) for k,v in dataset_test.labels.items())\n",
    "    assert os.path.isfile(file), \\\n",
    "    'file {} does not exist'.format(file)\n",
    "        \n",
    "    camera = cv2.VideoCapture(file)\n",
    "        \n",
    "    assert camera.isOpened(), \\\n",
    "    'Cannot capture source'\n",
    "    \n",
    "    _, frame = camera.read()\n",
    "    frame = resizer(frame)\n",
    "    height, width, _ = frame.shape\n",
    "    print(width, height)\n",
    "\n",
    "    if SaveVideo:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        fps = round(camera.get(cv2.CAP_PROP_FPS))\n",
    "        videoWriter = cv2.VideoWriter(\n",
    "            'converted_{}.mp4'.format(str(file.split(\"/\")[-1].split(\".\")[0])), \n",
    "            fourcc, fps, (width, height))\n",
    "\n",
    "    elapsed = int()\n",
    "    start = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        while camera.isOpened():\n",
    "            elapsed += 1\n",
    "            _, frame = camera.read()\n",
    "            \n",
    "            if frame is None:\n",
    "                print ('\\nEnd of Video')\n",
    "                break\n",
    "            img_tensor = transform(cv2.resize(frame,(width, height)))\n",
    "#             print(img_tensor.shape)\n",
    "            img_tensor.unsqueeze_(0)\n",
    "            \n",
    "            scores, classification, transformed_anchors = retinanet(img_tensor.cuda().float())\n",
    "\n",
    "            idxs = np.where(scores>0.5)\n",
    "            img = np.array(255 * unnormalize(img_tensor[0, :, :, :])).copy()\n",
    "\n",
    "            img[img<0] = 0\n",
    "            img[img>255] = 255\n",
    "\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "            img = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            for j in range(idxs[0].shape[0]):\n",
    "                bbox = transformed_anchors[idxs[0][j], :]\n",
    "                x1 = int(bbox[0])\n",
    "                y1 = int(bbox[1])\n",
    "                x2 = int(bbox[2])\n",
    "                y2 = int(bbox[3])\n",
    "                label_name = dataset_test.labels[int(classification[idxs[0][j]])]\n",
    "                color = COLORS[res[label_name]]\n",
    "                draw_caption(img, (x1, y1, x2, y2), label_name)\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), color, thickness=2)\n",
    "    \n",
    "            if SaveVideo:\n",
    "                videoWriter.write(img)\n",
    "            \n",
    "            if elapsed % 5 == 0:\n",
    "                sys.stdout.write('\\r')\n",
    "                sys.stdout.write('{0:3.3f} FPS'.format(\n",
    "                    elapsed / (time.time() - start)))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "    sys.stdout.write('\\n')\n",
    "    if SaveVideo:\n",
    "        videoWriter.release()\n",
    "    camera.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera(\"test_ads/abcd.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Testing Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(retinanet.state_dict(), \"state_dict/retinanet_state_dict.pth\")\n",
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406],\n",
    "   std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "transform=transforms.Compose([transforms.ToPILImage(), transforms.ToTensor(), normalize])\n",
    "\n",
    "import skimage\n",
    "img = skimage.io.imread(\"test_im/birthday.jpg\")\n",
    "print(img.shape)\n",
    "img1 = cv2.imread(\"test_im/birthday.jpg\")\n",
    "img1\n",
    "# img1 = resizer(img1)\n",
    "# fr = transform(img1)\n",
    "# print(fr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(a)\n",
    "print(a.shape)\n",
    "b = a.squeeze_(0)\n",
    "b = torch.transpose(b, 1, 2)\n",
    "b = torch.transpose(b, 0, 1)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
